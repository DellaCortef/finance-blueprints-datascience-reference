{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae13d40",
   "metadata": {},
   "source": [
    "# Supervised Learning: Models and Concepts\n",
    "\n",
    "Supervised learning is an area of machine learning in which the chosen algorithm attempts to fit a target using given input. A training data set containing labels is provided to the algorithm. Based on a massive set of data, it will learn a rule that it uses to predict labels on new observations. In other words, supervised learning algorithms receive historical data and the task of finding the relationship that has the best predictive power.\n",
    "\n",
    "There are two varieties of supervised learning algorithms: regression and classification algorithms. Regression-based supervised learning methods attempt to predict outputs based on input variables. Classification-based supervised learning methods identify the category to which a dataset belongs. Classification algorithms are based on probability, that is, the output is the category for which the algorithm finds the highest probability that the data set belongs to it. Regression algorithms, in contrast, estimate the output of problems that have an infinite number of solutions (continuous set of possible outputs).\n",
    "\n",
    "In the context of finance, supervised learning models represent one of the most widely used classes of machine learning models. Many algorithms that are widely applied in algorithmic trading use supervised learning models, as they can be trained efficiently, are relatively robust to noise in financial data, and have strong links to finance theory.\n",
    "\n",
    "Regression-based algorithms have been leveraged by academic and industry researchers to develop numerous asset pricing models. Such models are used to predict returns over multiple periods and to identify significant factors that drive returns on assets. There are many other use cases for regression-based supervised learning in portfolio management and derivatives pricing.\n",
    "\n",
    "Classification-based algorithms, on the other hand, have been pushed into several areas within finance that require predicting a categorical reaction. Among them, we have fraud detection, default prediction, credit score, directional prediction of movements in asset prices and buy/sell recommendations. There are many other use cases for classification-based supervised learning, such as portfolio management and algorithmic trading.\n",
    "\n",
    "---\n",
    "\n",
    "##### Topics that will be covered:\n",
    "\n",
    "- basic concepts about supervised learning models;\n",
    "- how to implement different supervised learning models in Python;\n",
    "- how to optimize models and identify their ideal parameters using grid search;\n",
    "- overfitting versus underfitting and bias versus variance;\n",
    "- strengths and weaknesses of the different supervised learning models;\n",
    "- how to use multiple models, deep learning and ANNs for regression and classification;\n",
    "- how to select a model based on several factors, including performance;\n",
    "- evaluation metrics for classification and regression models;\n",
    "- how to perform cross-validation;\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised Learning Models: Overview\n",
    "\n",
    "The problems of predictive classification modeling are different from those of predictive regression modeling in that classification is the task of predicting a discrete class label and regression is the task of predicting a continuous quantity. However, they both share the same concept of using known variables to make predictions, and there are many things that overlap between the models. Therefore, the classification and regression models are presented together.\n",
    "\n",
    "Some models can be used for both classification and regression with minor modifications. These are the *K-nearest neighbors*, the decision trees, the support vector, the ensemble bagging and boosting methods and the ANNs (including deep neural networks). However, some models, such as linear regression and logistic regression, cannot (at least not easily) be used for both types of problems.\n",
    "\n",
    "We will analyze the following details:\n",
    "\n",
    "- model theory;\n",
    "- implementation in **Scikit-learn** or **Keras**;\n",
    "- grid search for different models;\n",
    "- pros and cons of the models.\n",
    "\n",
    "### Linear Regression (Ordinary Least Squares)\n",
    "\n",
    "*Linear regression* (ordinary least squares regression - OLS) is perhaps one of the most well-known and understood algorithms in statistics and machine learning. Linear regression is a linear model, that is, a model that assumes a linear relationship between the input variables (*x*) and the single output variable (*y*). Your goal is to train a linear model to predict a new *y*, considering a previously unobserved *x*, with as few errors as possible.\n",
    "\n",
    "Our model will be a function that predicts *y*, given that *x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>i</sub>*:\n",
    "\n",
    "*y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + ... + β<sub>i</sub>x<sub>i</sub>*\n",
    "\n",
    "where, *β<sub>0</sub>* is called the intercept, and *β<sub>1</sub> ... β<sub>i</sub>* are the regression coefficients.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Training a model\n",
    "\n",
    "As we mentioned previously, training a model basically means recovering the model parameters while minimizing the cost (loss) function. The two steps to training a linear regression model are:\n",
    "\n",
    "*Define a cost function (or loss function)*\n",
    "\n",
    "- measures the level of inaccuracy present in the model predictions. The *sum of the squares of the residuals (SQR)*, as defined previously, measures the sum of the squares of the difference between the actual and predicted values, and is the cost function for the linear regression.\n",
    "\n",
    "- equation:\n",
    "    - $SQR = \\sum_{i=1}^{n} (y_i - β_0 - \\sum_{j=1}^{n} β_j $x_{ij}$)^2$\n",
    "    \n",
    "In this equation, $β_0$ is the interceptor; $β_j$ represents the coefficient; $β_1, ..., β_j$ are the regression coefficients; and *$x_{ij}$* represents the *$i^a$* observation and the *$j^a$* variable.\n",
    "\n",
    "*Find the parameters that minimize loss*\n",
    "\n",
    "- for example, making our model as accurate as possible. Graphically, in two dimensions, this results in a line of best fit. In higher dimensions, we would have hyperplanes with more dimensions. mathematically, we observe the difference between the actual data points *(y)* and our model's prediction ($\\hat{y}$). Find the root of these differences to avoid negative numbers and penalize large differences, then add them up to get the average. This is a metric of how well our data fits the line.\n",
    "\n",
    "#### Grid search\n",
    "\n",
    "The general idea of grid search is to create a grid with all possible combinations of hyperparameters and train the model using each of them. Hyperparameters are the external characteristics of the model, they can be considered model adjustments and are not estimated based on model parameters similar to the data. These hyperparameters are activated during the grid search to obtain better model performance.\n",
    "\n",
    "Due to its detailed search, grid search will certainly find the ideal parameter within the grid. The disadvantage is that the size of the grid grows exponentially with the addition of more parameters or values considered.\n",
    "\n",
    "The **GridSearchCV** class in the *model_selection* module of the **sklearn** package makes it easy to systematically evaluate all combinations of hyperparameter values that we would like to test.\n",
    "\n",
    "The first step is to create a model object. Then, we define a dictionary in which the keywords name the hyperparameters and the values list the settings of the parameter to be tested. For linear regression, the hyperparameter is *fit_intercept*, which is a Boolean variable that determines whether or not to calculate the *intercept* for this model. If set to False, no intercepts will be used in calculations:\n",
    "\n",
    "```Python\n",
    "model = LinearRegression()\n",
    "params_grid = {'fit_intercept': [True, False]}\n",
    "```\n",
    "\n",
    "The second step is to instantiate the **GridSearchCV** object and provide the estimator objective and parameter grid, as well as a scoring method and a cross-validation choice, for the initialization method. Cross-variation is a resampling procedure used to evaluate learning models and the score parameter is the model's evaluative metric.\n",
    "\n",
    "We can adjust GridSearchCV:\n",
    "\n",
    "```Python\n",
    "grid = GriSearchCV(estimator = model, param_grid = params_grid, scoring = 'r2', cv = fold)\n",
    "grid_result = grid.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Advantages and disadvantages\n",
    "\n",
    "In terms of advantages, linear regression is easy to understand and interpret. However, it may not work well when there is a non-linear relationship between predicted and predictor variables. Linear regression has a tendency to *overfit*, and when a large number of features are present, it may not handle irrelevant features well. It also requires that the data follow certain assumptions, such as the absence of multicollinearity. If the hypothesis is false, we will not be able to trust the results obtained.\n",
    "\n",
    "### Regularized Regression\n",
    "\n",
    "When a linear regression model contains many independent variables, its coefficients will not be well determined and the model will have a tendency to fit the training data (data used to create the model) extremely well, but to fit the test data (data used to test the quality level of the model) extremely well. This is known as overfitting or high variance.\n",
    "\n",
    "A popular technique for controlling overfitting is *regularization*, which involves adding a *penalty* term so that the error or loss function discourages the coefficients from reaching large values. In simple terms, regularization is the penalty mechanism that applies shrinkage to model parameters (bringing them close to zero), in order to create a model with greater prediction and interpretation accuracy. Regularized regression has two advantages over linear regression:\n",
    "\n",
    "*Predictive accuracy*\n",
    "\n",
    "- the performance of the model that works best with all the test data suggests that the model is trying to generalize from the training data. A model with too many parameters may attempt to fit specific noises to the chirping data. By shrinking or setting some coefficients to zero, we give up the ability to fit complex models (larger biases) in exchange for a more generalizable model (lower variance).\n",
    "\n",
    "*Interpretation*\n",
    "\n",
    "- a large number of forecasters can complicate the interpretation or communication of the overall picture of results. It may be preferable to sacrifice some details to limit the model to a smaller subset of parameters with the strongest effects.\n",
    "\n",
    "Common ways to regularize a linear regression model are these:\n",
    "\n",
    "*L1 or Lasso regularization*\n",
    "\n",
    "- *Lasso regularization* performs *L1 regularization* by adding a factor of the sum of the coefficients of absolute values in the cost function (SQR/RSS) to the linear regression, as mentioned previously. The equation for Lasso regularization can be represented as follows:\n",
    "\n",
    "$Cost Function$ = $SQR + \\gamma \\times \\sum_{j=1}^{p} |\\beta_{j}|$\n",
    "\n",
    "- L1 regularization can lead to zero coefficients (i.e., some features are completely neglected for the output evaluation). The higher the value of $\\gamma$, the more features are shrunk to zero. This can eliminate some features entirely and give us a subset of predictors, reducing the complexity of the model. Thus, Lasso regression not only helps in reducing overfitting but can also help in feature selection. Predictors not shrunk to zero mean that they are important, and, in this way, L1 regularization allows feature selection (sparse selection). The regularization parameter ($\\gamma$) can be controlled, and a zero value of *lambda* produces the basic linear regression equation.\n",
    "\n",
    "A lasso regression model can be built using the *Lasso* class from the **sklearn** Python package, as we will show below:\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "*L2 or Ridge regularization*\n",
    "\n",
    "- *Ridge regression* performs *L2 regularization* by adding a factor of the sum of the square of the coefficients in the cost function for the linear regression. The equation for Ridge regularization can be represented like this:\n",
    "\n",
    "$Cost Function$ = $SQR + \\gamma \\times \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
    "\n",
    "- Ridge regression places restrictions on coefficients. The penalty term ($\\gamma$) regularizes the coefficients so that, if they take on large values, the optimization function is penalized. Thus, Ridge regression shrinks the coefficients and helps reduce model complexity. Shrinking the coefficients leads to lower variance and a lower error value. Therefore, Ridge regression reduces the complexity of the model, but does not reduce the number of variables; it only diminishes its effect. When $\\gamma$ is closer to zero, the cost function becomes similar to the linear regression cost function. Therefore, the smaller the restrictions ($\\gamma$ $low$) on the features, the more the model will resemble a linear regression model.\n",
    "\n",
    "A Ridge regression model can be built using the *Ridge* class from the **sklearn** Python package, as shown in the following code:\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "*Elastic net*\n",
    "\n",
    "- *Elastic nets* acrescentam termos de regularização ao modelo, sendo uma combinação das regularizações L1 e L2, como mostra a equação:\n",
    "\n",
    "$Cost Function$ = $SQR + \\gamma \\times ((1 - \\alpha) / 2 \\times \\sum_{j=1}^{p} \\beta_{j}^{2} + \\alpha \\times \\sum_{j=1}^{p} |\\beta_{j}|$\n",
    "\n",
    "- in addition to establishing and choosing a value of $\\gamma$, elastic net also allows us to calibrate the alpha parameter, where $\\alpha = 0$ corresponds to ridge and $\\alpha = 1$ to lasso. Therefore, we can choose a $\\alpha$ value between *0* and *1* to optimize the elastic net. Effectively this will shrink some coefficients and set some to *0* for sparse selection.\n",
    "\n",
    "An elastic net regression model can be built using the *ElasticNet* class from the **sklearn** Python package, as we will show below:\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "For all regularized regressions, $\\gamma$ is the essential parameter to be calibrated during the grid search in Python. In an elastic net, $\\alpha$ can be an additional parameter to be calibrated.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "*Logistic Regression* is one of the most widely used algorithms for classification. The logistic regression model arises from the desire to model the probabilities of the output classes given a function that is linear in *x*, while ensuring that the output probabilities add up to 1 and remain between 0 and 1, which is what we expect from the probabilities.\n",
    "\n",
    "If we train a linear regression model with several examples in which *y = 0 or 1*, we may end up predicting some probabilities that are less than zero or greater than one, something that does not make sense. Therefore, we use a logistic regression model (or *logit* model), which is a modification of linear regression that guarantees to present as output a probability between 0 and 1, when applying the $sigmoid^2$ function.\n",
    "\n",
    "The following equation shows the logistic regression model. Similar to linear regression, input values *(X)* are combined linearly using weights or coefficient values to predict an output value *(y)*. The output of the equation is a probability that is transformed into a binary value (*0* or *1*) to obtain the model's prediction:\n",
    "\n",
    "$$\n",
    "y = \\frac{exp^{(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}{1 + exp^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "$y$ is the predicted output, $\\beta_0$ is the bias or intercept term, and $\\beta_1$ is the coefficient for the single input value $X$. Each column in the input data has an associated coefficient $\\beta$ (a constant real value) that must be learned from the training data.\n",
    "\n",
    "In logistic regression, the cost function is basically a metric of how often we predict a when the true response is zero, or vice versa. Logistic regression coefficients are trained using techniques such as the maximum likelihood estimator (or MLE) to predict values ​​close to *1* for the default class and close to *0* for the other class.\n",
    "\n",
    "A logistic regression model can be built using the *LogisticRegression* class from the **sklearn** Python package, as shown below:\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "*Regularization (penalty* in **sklearn**)\n",
    "- like linear regression, logistic regression can have regularization, which can be *L1*, *L2*, or *elasticnet*. The values in the **sklearn** library are *[l1, l2, elasticnet]*\n",
    "\n",
    "*Regularization strength (C* in **sklearn**)\n",
    "- this parameter controls the strength of the regularization. Good penalty parameter values can be *[100, 10, 1.0, 0.1, 0.01]*\n",
    "\n",
    "#### Advantages and disadvantages\n",
    "\n",
    "Considering the advantages, the logistic regression model is easy to implement, has good interpretability, and performs very well on linearly separable classes. The output of the model is a probability, which provides more insights and can be used for ranking. The model has a small number of hyperparameters. Although there may be a risk of overfitting, this can be addressed using *L1/L2* regularization, similar to how we address overfitting in linear regression models.\n",
    "\n",
    "In terms of disadvantages, the model may overfit when fed with a large number of features. Logistic regression can only learn linear functions and is less suited to complex relationships between features and target variables. Furthermore, it may not handle irrelevant features well, especially if they are strongly correlated.\n",
    "\n",
    "### Support Vector Machine\n",
    "\n",
    "The goal of the *support vector machine* (SVM) algorithm is to maximize the margin, which is defined as the distance between the separating hyperplane (or decision boundary) and the training samples that are closest to this hyperplane, the so-called support vectors. The margin is calculated as the perpendicular distance from the line to only the closest points. Thus, the SVM calculates a boundary with maximum margin that leads to a homogeneous division of all data points.\n",
    "\n",
    "In practice, the data is messy and cannot be separated perfectly with a hyperplane. The constraint of maximizing the margin of the line separating the classes must be relaxed. A change is made to allow some training data points to violate the separating line. An additional set of coefficients is introduced, which gives the margin a relaxation in each dimension. A calibration parameter is introduced, simply called *C*, which defines the magnitude of relaxation allowed in all dimensions. The larger the value of *C*, the more violations of the hyperplane are allowed.\n",
    "\n",
    "In some cases, it is not possible to find a hyperplane or a linear decision boundary, and kernels are used. A kernel is simply a transformation of the input data that allows the SVM algorithm to process the data more easily. Using kernels, the data is projected into a higher dimension to classify it better.\n",
    "\n",
    "SVM is used for both classification and regression. This is possible by converting the original optimization problem into a dual problem. For regression, the trick is to reverse the objective. Instead of trying to fit the widest possible street between two classes while limiting margin violations, SVM regression tries to fit as many instances as possible on the street while limiting margin violations. The width of the street is controlled by a hyperparameter.\n",
    "\n",
    "SVM regression and classification models can be built using the Python **sklearn** package, as shown in the following code:\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.svm import SVR\n",
    "model = SVR()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The following key parameters are provided in the sklearn implementation of SVM and can be adjusted during grid search:\n",
    "\n",
    "*Kernels*\n",
    "- the choice of kernel controls how the input variables will be projected. There are many kernel options, but *linear* and *RFB* are the most common;\n",
    "\n",
    "*Penalty*\n",
    "- the penalty parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For larger values of the penalty parameter, the optimization will choose a hyperplane with a smaller margin. Good values can have a logarithmic scale from 10 to 1,000.\n",
    "\n",
    "#### Advantages and disadvantages\n",
    "\n",
    "In terms of advantages, the SVM is quite robust against overfitting, especially in a higher-dimensional space. It handles nonlinear relationships very well, with many kernel options to choose from. Furthermore, there is no distributional log for the data.\n",
    "\n",
    "Considering the disadvantages, SVM can be inefficient to train and requires a lot of memory to run and calibrate. It does not perform well on large data sets. It requires feature scaling of the data. There are also many hyperparameters and their meanings are often not intuitive.\n",
    "\n",
    "### K-Nearest Neighbors\n",
    "\n",
    "*K-nearest neighbors* (KNN) is considered a \"lazy learner\" because there is no learning required by the model. For a new data point, predictions are made by searching the entire training set for the *K* most similar instances (the neighbors) and summarizing the output variable for these *K* instances.\n",
    "\n",
    "To determine which *K* instances in the training data set are most similar to the new input, a distance measure is used. The most popular is the *Euclidean distance*, which is calculated as the square root of the sum of the squared differences between a point *a* and a point *b* over all input attributes *i* and is represented as:\n",
    "\n",
    "\n",
    "$$ \n",
    "d(a, b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n",
    "$$\n",
    "\n",
    "A distância euclidiana é uma boa métrica de distância a ser usada se as variáveis de entrada forem semelhantes em tipo.\n",
    "\n",
    "Another distance metric is the *Manhattan distance*, where the distance between point *a* and point *b* is represented as:\n",
    "\n",
    "$$\n",
    "d(a, b) = {\\sum_{i=1}^{n} |a_i - b_i|}\n",
    "$$\n",
    "\n",
    "This is a good metric to use if the input variables are not similar in type. The steps of KNN can be summarized as follows:\n",
    "\n",
    "1. Choose the number of *K* and a distance metric;\n",
    "2. Find the *K-nearest neighbors* of the sample you want to classify;\n",
    "3. Assign the class label by majority vote.\n",
    "\n",
    "KNN regression and classification models can be built using the Python **sklearn** package, as demonstrated:\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The following key parameters are present in the **sklearn** implementation of KNN and can be adjusted during grid search:\n",
    "\n",
    "*Number of neighbors*\n",
    "- the most important hyperparameter for KNN is the number of neighbors (*n_neighbors*). Good values ​​are between 1 and 20;\n",
    "\n",
    "*Distance metric*\n",
    "- it may also be interesting to test different distance metrics to choose the composition of the neighborhood. Good values are *euclidean* and *manhattan*.\n",
    "\n",
    "#### Advantages and disadvantages\n",
    "\n",
    "In terms of advantages, there is no training involved and therefore no learning phase. Since the algorithm does not require training before making predictions, new data can be added seamlessly without impacting the accuracy of the algorithm. It is intuitive and easy to understand. The model handles multi-class classification naturally and can learn complex decision boundaries. KNN is effective if the training data is large. It is also robust to noisy data and there is no need to filter out outliers.\n",
    "\n",
    "Considering disadvantages, the distance metric to choose is not obvious and in many cases difficult to justify. KNN performs poorly with high-dimensional datasets. It is expensive and slow to predict new instances as the distance to all neighbors has to be recalculated. It is sensitive to noise in the dataset. We need to manually insert missing values and remove outliers. Furthermore, feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset; otherwise, it may generate wrong predictions.\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "The goal of the *linear discriminant analysis* (LDA) algorithm is to project data into a space with fewer dimensions so that class separability is maximized and within-class variance is minimized.\n",
    "\n",
    "During training of the LDA model, the statistical properties (mean and covariance matrix) of each class are computed. They are estimated based on the following assumptions about the data:\n",
    "\n",
    "- the data has a normal distribution, so that each variable has the shape of a bell curve when plotted;\n",
    "\n",
    "- each attribute has the same variance, and the values of each variable vary around the mean by approximately the same amount.\n",
    "\n",
    "To make predictions, LDA estimates the probability that a new set of inputs belongs to each class. The output class is the one with the highest probability.\n",
    "\n",
    "#### Python implementation and hyperparameters\n",
    "\n",
    "The LDA classification model can be built using the **sklearn** package, as follows:\n",
    "\n",
    "```Python\n",
    "from sklean.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model = LinearDiscriminantAnalysis ()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "The key hyperparameter for the LDA model is the number of components for dimensionality reduction, represented by *n_components* in **sklearn**.\n",
    "\n",
    "#### Advantages and disadvantages\n",
    "\n",
    "The advantages are that LDA is a relatively simple model, with a quick and easy implementation. The disadvantages are that it requires feature scaling and involves complex matrix operations.\n",
    "\n",
    "### Classification and Regression Trees\n",
    "\n",
    "In general terms, the purpose of a tree-building analysis algorithm is to determine a set of logical *if/then* (split) conditions that allow for accurate prediction or classification of cases. Classification and regression trees are attractive models if interpretability is important to us. We can think of this model as decomposing our data and making decisions based on a series of questions asked. This algorithm is the foundation of ensemble methods such as random forest and gradient boosting.\n",
    "\n",
    "#### Representation\n",
    "\n",
    "The model can be represented by a *binary tree* or *decision tree*, where each node is an input variable *x* with a bifurcation point and each leaf contains an output variable *y* for each prediction.\n",
    "\n",
    "##### Learning a CART model\n",
    "\n",
    "Creating a binary tree is actually a process of splitting the input space. A *greedy approach* called *recursive binary split* is used to split the space. This is a numerical procedure in which all the values ​​are aligned and different split points are tested using a cost (loss) function. The split with the best cost (lowest cost, since we minimize it) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner (i.e. the best split point will be chosen each time).\n",
    "\n",
    "For predictive regression modeling problems, the cost function that is minimized to choose the split points is the *sum of squared errors* over all training samples.\n",
    "\n",
    "${\\sum_{i=1}^{n} (y_i - predição_i)^2}$\n",
    "\n",
    "where $y_i$ is the output for the training sample and prediction is the predicted output. For classification, the *Gini coefficient* is used; it gives an indication of the purity level of the leaf nodes (i.e., whether the training data assigned to each node is well mixed) and is defined as:\n",
    "\n",
    "$G = {{\\sum_{i=1}^{n} P_k^* (1 - p_k)}}$\n",
    "\n",
    "where *G* is the Gini cost across all classes, and *P_k* is the number of training instances with the class k of interest. A node that has all classes of the same type (perfect class purity) will have *G = 0*, while a node that has a *50 - 50* split of classes for a binary classification problem (worst purity) will have *G = 0.5*.\n",
    "\n",
    "##### Stopping Criteria\n",
    "\n",
    "The recursive binary split procedure described above needs to know when to stop splitting as it descends the tree of training data. The most common stopping procedure is to use a minimum count of the number of training instances assigned to each leaf node. If the count is less than some minimum, then the split is not accepted and the node is considered the last one.\n",
    "\n",
    "##### Pruning the tree\n",
    "\n",
    "The stopping criterion is important, as it greatly influences the performance of the tree. Pruning can be used after the tree has learned to further improve performance. The complexity of a decision tree is defined as the number of splits in it. Simpler trees are preferred, as they are faster to execute and easier to understand, consume less memory and storage during processing, and are less likely to overfit the data. The quickest and simplest method of pruning is to analyze each leaf node in the tree and evaluate the effect of removing it using a test set. A leaf node is removed only if it results in a drop in the overall cost function on the entire test set. The removal of nodes can be stopped when no further improvements can be made.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "CART regression and classification models can be built using the **sklearn** package, as shown in the following code:\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "##### Hyperparameters\n",
    "\n",
    "CART has many hyperparameters. However, the main one is the maximum depth of the tree model, which is the number of components for dimensionality reduction and is represented by *max_depth* in the **sklearn** package. Good values can range from *2 to 30*, depending on the number of features in the data.\n",
    "\n",
    "##### Advantages and disadvantages\n",
    "\n",
    "In terms of advantages, CART is easy to interpret and can be adapted to learn complex relationships. It requires little data preparation, and the data usually does not need to be scaled. Feature importance is built in due to the way decision nodes are created. It performs well on large data sets. It works on both regression and classification problems.\n",
    "\n",
    "In terms of disadvantages, CART has a tendency to overfit unless pruning is used. It can be very robust, meaning that small changes in the training set can lead to quite significant differences in the hypothesis function that is learned. In general, it performs worse than ensemble models.\n",
    "\n",
    "### Ensemble Models\n",
    "\n",
    "The goal of ensemble models is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier. For example, assuming we collected predictions from ten experts, ensemble methods would allow us to strategically combine their predictions and make a prediction that is more accurate and robust than each expert’s individual predictions.\n",
    "\n",
    "The two most popular ensemble methods are bagging and boosting. Bagging (or bootstrap aggregation) is an ensemble technique for training multiple individual models in parallel. Each model is trained on a random subset of data. Boosting, on the other hand, is an ensemble technique for training multiple models sequentially. This is done by building a model that tries to correct for errors first. Models are added until the training set predicts perfectly or until a maximum number of models have been added. Each individual model learns from the errors made by the previous model. Like decision trees themselves, bagging and boosting can be used for classification and regression problems.\n",
    "\n",
    "By combining individual models, the ensemble model tends to be more flexible (less bias) and less sensitive to the data (less variance). Ensemble methods combine multiple, simpler algorithms to achieve better performance.\n",
    "\n",
    "We will look at random forest, AdaBoost, gradient boosting, and extraneous trees, along with their implementations.\n",
    "\n",
    "#### Random forest\n",
    "\n",
    "A random forest is a fine-tuned version of a decision tree with bagging. To understand such an algorithm, let's first define the bagging algorithm. Assuming we have a dataset with a thousand instances, the bagging steps are:\n",
    "\n",
    "1. Create many random subsamples from our dataset;\n",
    "2. Train a CART model with each sample;\n",
    "3. Given a new dataset, calculate the average prediction of each model and aggregate the predictions by each tree to assign the final label by majority vote.\n",
    "\n",
    "A problem with decision trees like CART is that they are greedy. They choose the variable to split using a greedy algorithm that minimizes the error. Even after bagging, decision trees can have many structural similarities and result in high correlation in their predictions. Combining predictions from multiple models into ensembles works best if the predictions from the submodels are uncorrelated or at most weakly correlated. Random forest changes the learning algorithm in such a way that the resulting predictions from all subtrees are less correlated.\n",
    "\n",
    "In CART, when selecting a split point, the learning algorithm is allowed to analyze all variables and all variable values to select the optimal split point. The random forest algorithm changes this procedure so that each subtree can access only a random sample of features when selecting split points. The number of features that can be searched at each split point (*m*) must be specified as a parameter to the algorithm.\n",
    "\n",
    "As bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point. In regression problems, this might be the drop in the sum of squared errors, and in classification, it might be the Gini cost. The bagging method can provide feature importance by calculating and averaging the drop in the error function for individual variables.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Random tree models for regression and classification can be built by following the code below:\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "##### Hyperparameters\n",
    "\n",
    "Some of the hyperparameters that are present in **sklearn**'s implementation of random forest and that can be adjusted during grid search are:\n",
    "\n",
    "*Maximum number of features* (max_depth)\n",
    "- this is the most important parameter. It is the number of random features to be sampled at each split point. You can use a range of integer values, such as 1 to 20 or 1 to half the number of input features.\n",
    "\n",
    "*Number of estimators* (n_estimators)\n",
    "- this represents the number of trees. Ideally, it should be increased until no further improvement is seen in the model. Good values can be a logarithmic scale from 10 to 1.000.\n",
    "\n",
    "##### Advantages and disadvantages\n",
    "\n",
    "The random forest algorithm or model has gained enormous popularity in machine learning applications over the past decade due to its good performance, scalability, and ease of use. It is flexible and naturally assigns feature importance scores, so it can handle redundant feature columns. It is scalable to larger data sets and is generally resistant to overfitting. The algorithm does not need the data to be scaled and can model a non-linear relationship.\n",
    "\n",
    "In terms of disadvantages, random forest can seem like a black-box approach, as we have very little control over what the model does and it can be difficult to interpret the results. While random forest does a good job with classification, it may not be as good for regression problems, as it does not provide a continuous and accurate prediction. In the case of regression, it does not predict beyond the range of the training data and may overfit on datasets that are particularly noisy.\n",
    "\n",
    "#### Extra trees\n",
    "\n",
    "*Extra trees*, also known as *extremely randomized tress*, is a variation of random forest; it creates multiple trees and splits the nodes using subsets of features, as in a random forest. However, unlike random forest, where observations are made with replacement, observations are made without replacement. Thus, there is no repetition of observations.\n",
    "\n",
    "In addition, random forest selects the best split to convert the parent node into the two most homogeneous child nodes. However, extra trees selects a random split to split the parent node into two random child nodes. In extra trees, the randomness does not come from bootstrapping the data, but from random splits of all observations.\n",
    "\n",
    "In real-world cases, the performance is comparable to a regular random forest, sometimes slightly better. The advantages and disadvantages of extra trees are similar to those of random forest.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Extra trees models for regression and classification can be built using **sklearn**, as shown in the following code snippet. The hyperparameters are similar to those for the random forest shown earlier:\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "#### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "*Adaptive Boosting* or *AdaBoost* is a boosting technique in which the basic idea is to sample predictors sequentially, and each subsequent model tries to correct the errors of its predecessor. In each iteration, the AdaBoost algorithm changes the sampling distribution by modifying the weights attached to each of the instances. It increases the weights of the erroneously predicted instances and decreases those of the correctly predicted instances.\n",
    "\n",
    "The steps of the AdaBoost algorithm are:\n",
    "\n",
    "1. Initially, all the observations are given equal weights;\n",
    "2. A model is built with a subset of data and using this model, predictions are made with the entire data set. The errors are calculated by comparing the predictions and the actual values;\n",
    "3. During the creation of the next model, higher weights are given to the data points that were incorrectly predicted. The weights can be determined using the error value. For example, the larger the error, the more weight is given to the observation;\n",
    "4. This process is repeated until the error function does not change or until the upper limit on the number of estimators is reached.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "AdaBoost regression and classification models can be built with **sklearn**, as shown below:\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "##### Hyperparameters\n",
    "\n",
    "Some of the main hyperparameters that are present in the **sklearn** implementation of AdaBoost and that can be adjusted during grid search are:\n",
    "\n",
    "*Learning rate* (learning_rate)\n",
    "- decreases the contribution of each classifier/regressor. It can be considered on a logarithmic scale. The sample values for grid search can be 0.001, 0.01, and 0.1.\n",
    "\n",
    "*Number of estimators* (n_estimators)\n",
    "- this represents the number of trees. Ideally, it should be increased until no further improvement is seen in the model. Good values can be a logarithmic scale from 10 to 1,000.\n",
    "\n",
    "##### Advantages and disadvantages\n",
    "\n",
    "In terms of advantages, AdaBoost has a high degree of accuracy. It can achieve similar results to other models, requiring much less parameter or configuration adjustments. The algorithm does not need data to be scaled and can model a non-linear relationship.\n",
    "\n",
    "In terms of disadvantages, AdaBoost is very time-consuming to train. It can be sensitive to noisy data and outliers, and data imbalance leads to a decrease in classification accuracy.\n",
    "\n",
    "#### Gradient boosting method\n",
    "\n",
    "*Gradient boosting method* (GBM) is another boosting technique similar to AdaBoost, where the general idea is to sample the predictors sequentially. Gradient boosting works by sequentially adding the previous underfitted predictions to the ensemble, ensuring that the errors made previously are corrected.\n",
    "\n",
    "The steps of the gradient boosting algorithm are as follows:\n",
    "\n",
    "1. A model (which can be referred to as the first weak learner) is built with a subset of the data. Using this model, predictions are made with the entire data set;\n",
    "2. Errors are calculated by comparing the predictions and the actual values, and the loss is calculated using the loss function;\n",
    "3. A new model is created using the errors from the previous step as the target variable. The goal is to find the best split in the data to minimize the error. The predictions made by this new model are combined with the predictions from the previous one. New errors are calculated using this predicted value and the actual value;\n",
    "4. This process is repeated until the error function does not change or until the upper limit of the number of estimators has been reached.\n",
    "\n",
    "Unlike AdaBoost, which adjusts the weights of the instances in each iteration, this method tries to adjust the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Gradient boosting models for regression and classification can be built using the **sklearn** package, as shown in the following code. The hyperparameters are similar to those for AdaBoost, shown previously.\n",
    "\n",
    "```Python\n",
    "# Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "##### Advantages and disadvantages\n",
    "\n",
    "Considering the advantages, the gradient boosting method is robust to missing data, highly correlated features, and irrelevant features, just like the random forest. It naturally assigns feature importance scores, with slightly better performance than the random flower. The algorithm does not need data to be scaled and can model a non-linear relationship.\n",
    "\n",
    "As for the disadvantages, it may be more prone to overfitting than the random forest, since the main purpose of the boosting approach is to reduce bias, not variance. It has many hyperparameters to calibrate, so model development may not be fast. Also, feature importance may not be robust in the training dataset.\n",
    "\n",
    "### Modelos Baseados em RNAs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c1276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
