{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8b22fa",
   "metadata": {},
   "source": [
    "Regarding the platforms used for machine learning, there are many algorithms and programming languages. However, the Python ecosystem is one of the most dominant and fastest growing in machine learning.\n",
    "\n",
    "Given its popularity and high adoption rate, we will use Python as our main programming language. First, we will look at the details of the Python packages used for machine learning, followed by the model development steps in the Python framework.\n",
    "\n",
    "## Why Python?\n",
    "\n",
    "These are some of the reasons for Python's popularity:\n",
    "\n",
    "- high-level syntax (compared to lower-level languages ​​like C, Java, and C++). Applications can be developed by writing fewer lines of code, making Python attractive to both beginners and advanced programmers;\n",
    "- efficient development life cycle;\n",
    "- large collection of community-run and open-source libraries;\n",
    "- strong portability.\n",
    "\n",
    "Python's simplicity attracts many developers who create new libraries for machine learning, leading to its strong adoption.\n",
    "\n",
    "## Python Libs for Machine Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data Manipulation and Transformation**\n",
    "\n",
    "*NumPy* (https://numpy.org)\n",
    "\n",
    "    - provides support for large and multidimensional arrays, as well as an extensive collection of mathematical functions;\n",
    "\n",
    "*Pandas* (https://pandas.pydata.org)\n",
    "\n",
    "    - a library for data manipulation and analysis. Among other features, it offers data structure to work with tables and tools to manipulate them;\n",
    "    \n",
    "<br>\n",
    "\n",
    "**Machine Learning and Statistical Analysis**\n",
    "\n",
    "*SciPy* (https://www.scipy.org)\n",
    "\n",
    "    - the combination of **NumPy**, **Pandas** and **Matplotlib** is commonly known as **SciPy**, which is an ecosystem of Python libraries for mathematics, science and engineering;\n",
    "    \n",
    "*Scikit-learn* (https://scikit-learn.org)\n",
    "\n",
    "    - a machine learning library offering a wide range of algorithms and utilities;\n",
    "    \n",
    "*StatsModels* (https://www.statsmodels.org)\n",
    "\n",
    "    - a Python module that provides classes and functions for estimating numerous statistical models, as well as for conducting statistical tests and exploring statistical data;\n",
    "    \n",
    "*TensorFlow* (https://www.tensorflow.org) and *Theano* (https://deeplearning.net/software/theano)\n",
    "\n",
    "    - dataflow libraries that facilitate work with neural networks;\n",
    "    \n",
    "*Keras* (https://keras.io)\n",
    "\n",
    "    - a library of artificial neural networks that can act as a simplified interface for the **TensorFlow*/Theano* packages;\n",
    "    \n",
    "<br>\n",
    "\n",
    "**Data Visualization**\n",
    "\n",
    "*Matplotlib* (https://matplotlib.org)\n",
    "\n",
    "    - a plotting library that allows you to create 2D graphs and plots;\n",
    "\n",
    "*Seaborn* (https://seaborn.pydata.org)\n",
    "\n",
    "    - a data visualization library based on Matplotlib. Provides a high-level interface for creating attractive and informative statistical graphs;\n",
    "    \n",
    "<br>\n",
    "\n",
    "## Machine Learning Crisp-DM Model\n",
    "\n",
    "The figure below shows a general idea of a simple machine learning template in seven steps, which can be used to start any machine learning model in Python.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1024/1*sicHaDLyHRGuJm9eZTaHNw.png\" width=\"600\">\n",
    "    <figcaption>Step by step to develop a machine learning model</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Blueprint de Desenvolvimento de Modelo\n",
    "\n",
    "Now we will detail each step of the model development process:\n",
    "\n",
    "**1. Problem definition**\n",
    "The first step in any project is defining the problem. Powerful algorithms can be used to solve it, but the results will be of no use if the wrong problem is solved.\n",
    "The following framework should be used to define the problem:\n",
    "\n",
    "    1. describe the problem informally and formally. List similar assumptions and problems;\n",
    "    2. list the motivation for solving the problem, the benefits brought by the resolution and how it will be used;\n",
    "    3. describe how the problem would be solved using domain knowledge.\n",
    "\n",
    "\n",
    "**2. Loading data and libs**\n",
    "The second step gives you everything you need to start working on the problem. This includes loading libraries, packages, and functions required for model development.\n",
    "\n",
    "**2.1. Loading the libs**\n",
    "```Python\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "```\n",
    "Details of libraries and modules for specific functionalities can be found on each page.\n",
    "\n",
    "**2.2. Loading data**\n",
    "The following items must be checked and removed before data is loaded:\n",
    "\n",
    "    - column headers;\n",
    "    - comments or characters;\n",
    "    - delimiter.\n",
    "\n",
    "There are many ways to load data. Some of the most common are:\n",
    "\n",
    "```Python\n",
    "# Upload CSV files with Pandas\n",
    "from pandas import read_csv\n",
    "filename = 'xpto.csv'\n",
    "data = read_csv(filename, names=names)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Upload files from a URL\n",
    "url = 'https://goo.glvhm1eU'\n",
    "names = ['age', 'class']\n",
    "data = read_csv(url, names=names)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Load files using pandas_darareader\n",
    "import pandas_datareader.data as web\n",
    "ccy_tickers = ['DEXJPUS', 'DEXUSUK']\n",
    "idx_tickers = ['SP500', 'DJIA', 'VIXCLS']\n",
    "\n",
    "stk_data = web.DataReader(stk_tickers, 'yahoo')\n",
    "ccy_data = web.DataReader(ccy_tickers, 'fred')\n",
    "idx_data = web.DataReader(idx_tickers, 'fred')\n",
    "```\n",
    "\n",
    "**3. Exploratory data analysis**\n",
    "In this step, we analyze the data set.\n",
    "\n",
    "**3.1. Descriptive statistics**\n",
    "Understanding the dataset is one of the most important processes in model development. The steps to do this include:\n",
    "\n",
    "    1. view the raw data;\n",
    "    2. evaluate the dimensions of the set;\n",
    "    3. evaluate the types of data attributes;\n",
    "    4. summarize the distribution, descriptive statistics, and relationship between variables in the data set.\n",
    "    \n",
    "These steps are demonstrated below:\n",
    "\n",
    "```Python\n",
    "# View data\n",
    "set_option('display.width', 100)\n",
    "dataset.head(1)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Evaluate dataset dimensions\n",
    "dataset.shape\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Evaluate data attribute types\n",
    "set_option('display.max_rows', 500)\n",
    "dataset.dtypes\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Summarize data using descriptive statistics\n",
    "set_option('precision', 3)\n",
    "dataset.describe()\n",
    "```\n",
    "\n",
    "**3.2. Data visualization**\n",
    "The quickest way to learn more about data is to visualize it. Visualization involves independently understanding each attribute of the dataset.\n",
    "\n",
    "Types of charts:\n",
    "\n",
    "*Univariates*\n",
    "- histograms and density graphs;\n",
    "\n",
    "*Multivariates*\n",
    "- correlation matrix and scatter plot;\n",
    "\n",
    "Below are Python code examples for univariate graph types:\n",
    "\n",
    "```Python\n",
    "# Univariate: histogram\n",
    "from matplotlib import pyplot\n",
    "dataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(10,4))\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Univariate: density plot\n",
    "from matplotlib import pyplot\n",
    "dataset.plot(kind='density', subplots=True, layout=(3, 3), sharex=False, legend=True, fontsize=1, figsize=(10,4))\n",
    "pyplot.show()\n",
    "```\n",
    "\n",
    "Below are Python code examples for multivariate graph types:\n",
    "\n",
    "```Python\n",
    "# Multivariate: correlation matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "correlation = dataset.corr()\n",
    "pyplot.figure(figsize=(5,5))\n",
    "pyplot.title('Correlation Matrix')\n",
    "sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Multivariate: scatter plot\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dataset)\n",
    "```\n",
    "\n",
    "**4. Data preparation**\n",
    "Data preparation is the preprocessing step in which data from one or more sources is cleaned and transformed to improve its quality before it is used.\n",
    "\n",
    "**4.1. Data cleaning**\n",
    "In machine learning modeling, bad data can be costly. Data cleansing involves checking the following:\n",
    "\n",
    "*Validity*\n",
    "- type, range, ...;\n",
    "\n",
    "*Accuracy*\n",
    "- the degree to which the data are close to true values;\n",
    "\n",
    "*Completeness*\n",
    "- the degree to which all necessary data are known;\n",
    "\n",
    "*Uniformity*\n",
    "- the degree to which data are specified using the same unit of measurement;\n",
    "\n",
    "Different options for cleaning data include:\n",
    "\n",
    "*Remove \"NA\" values ​​from data*\n",
    "```Python\n",
    "dataset.dropna(axis=0)\n",
    "```\n",
    "\n",
    "*Fill \"NA\" with 0*\n",
    "```Python\n",
    "dataset.fillna(0)\n",
    "```\n",
    "\n",
    "*Fill in \"NA\" with the column average*\n",
    "```Python\n",
    "dataset['col'] = dataset['col'].fillna(dataset['col'].mean())\n",
    "```\n",
    "\n",
    "**4.2. Feature selection**\n",
    "The features of the data used to train machine learning models have a huge influence on performance. Irrelevant or partially relevant features can negatively impact model performance. Feature selection is a process in which the features in the data that contribute most to the prediction of the variable or output are automatically selected.\n",
    "\n",
    "The benefits of performing feature selection before modeling are:\n",
    "\n",
    "*Reduced overfitting (overfitting)*\n",
    "- less redundant data means fewer opportunities for the model to make decisions based on noise;\n",
    "\n",
    "*Improves performance*\n",
    "- less misleading data means better modeling performance;\n",
    "\n",
    "*Reduction in training time and memory volume*\n",
    "- less data means faster training and less memory volume;\n",
    "\n",
    "The following sample feature is an example that demonstrates when two best features are selected using the *SelectKBest* function in **sklearn**. This function ranks the features using an underlying function and then removes all but the highest ranked *k* feature:\n",
    "\n",
    "```Python\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "bestfeatures = SelectKBest(k=5)\n",
    "dfscores = pd.DataFrame(X.fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns) \n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "print(featureScores.nlargest(2,'Score'))\n",
    "```\n",
    "\n",
    "When features are irrelevant, they should be removed. This is presented below:\n",
    "\n",
    "```Python\n",
    "# Removing old features\n",
    "dataset.drop(['Feature1', 'Feature2', 'Feature3'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "**4.3. Data transformation**\n",
    "Many machine learning algorithms make assumptions about data. It is good practice to perform data preparation in such a way that it best exposes it to machine learning algorithms. This can be accomplished by data transformation.\n",
    "\n",
    "The different approaches to doing this are:\n",
    "\n",
    "*Resizing*\n",
    "- when data spans attributes with varying scales, many machine learning algorithms can benefit from *rescaling* all attributes to the same scale. Attributes are typically scaled in the range between zero and one. This is useful for optimization algorithms used in core machine learning algorithms and also helps speed up calculations in an algorithm:\n",
    "\n",
    "```Python\n",
    "from skear.preprocessing import MinMaxScaler\n",
    "sclaer = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = pd.DataFrame(scaler.fit_transform(X))\n",
    "```\n",
    "\n",
    "*Standardization*\n",
    "- standardization is a useful technique for transforming attributes to a normal distribution with a mean of 0 and a standard deviation of 1. It is best suited for techniques that assume that the input variables represent a normal distribution:\n",
    "\n",
    "```Python\n",
    "from sklearn.preprocessinf import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "StandardizedX = pd.DataFrame(scaler.fit_transform(X))\n",
    "```\n",
    "\n",
    "*Normalization*\n",
    "- normalization refers to scaling each observation (record) to have a length of 1 (called the unit or vector norm). This preprocessing method can be useful for sparse datasets of attributes of varying scales, especially to apply in algorithms that give different weights to input values:\n",
    "\n",
    "```Python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X)\n",
    "NormalizedX = pd.DataFrame(scaler.fit_transform(X))\n",
    "```\n",
    "\n",
    "**5. Evaluation models**\n",
    "After we have estimated the performance of our algorithm, we can retrain the final algorithm with the entire training data set and make it ready for operational use. The best way to do this is to evaluate your performance with a new set of data. Different machine learning techniques require different evaluation metrics. In addition to model performance, several other factors such as simplicity, interpretability and training time are considered when selecting a model.\n",
    "\n",
    "**5.1. Division for training and testing**\n",
    "The simplest method we can use to evaluate the performance of a machine learning algorithm is to use different datasets for training and testing. We can take our original dataset and divide it into two parts: train the algorithm with the first part; make predictions with the second; and evaluate predictions against expected results. The size of the split can depend on the size and details of the dataset, although it is common to use 80% of the data for training and 20% of the data for testing. Differences in training and testing data sets can bring significant results to accuracy estimation. Data can be easily split into training and testing sets using the *train_test_split* function available in **sklearn**:\n",
    "```Python\n",
    "# Split the dataset\n",
    "validation_size = 0.2\n",
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = validation_size, random_state = seed)\n",
    "```\n",
    "\n",
    "**5.2. Identify evaluation metrics**\n",
    "Choosing which metric to use to evaluate machine learning algorithms is very important. A fundamental aspect of the evaluation metric is its ability to discern the different results of the model.\n",
    "\n",
    "**5.3. Compare models and algorithms**\n",
    "It is an art and a science to select a machine learning model or algorithm. There is no panacea. There are several factors beyond the performance model that can impact the decision to choose a machine learning algorithm.\n",
    "\n",
    "Let's understand the model comparison process with a simple example. We define two variables, *X* and *Y*, and try to create a model to predict *Y* using *X*. As a first step, the data is divided into training and testing groups, as mentioned previously:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "validation_szie = 0.2\n",
    "seed = 42\n",
    "X = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "Y = X - 2 * (X ** 2) + 0.5 * (X ** 3) + np.exp(-X) + np.random.normal(-3, 3, 20)\n",
    "X = X[:, np.newaxis]\n",
    "Y = Y[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = validation_size, random_state = seed)\n",
    "```\n",
    "\n",
    "We have no idea which algorithm will do well in this problem. Let's schedule our test now. We will use two models - a linear regression and a polynomial regression to fit *Y* to *X*. We will evaluate the algorithms using the *Root Mean Squared Error (RMSE)* metric, which is one of the model's performance measures. RMSE will give us a general idea of the level of error of all forecasts:\n",
    "\n",
    "```Python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_linear = r2_score(y_train, y_pred)\n",
    "print(\"RMSE for Linear Regression:\" , rmse_linear)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y_train)\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_poly_pred))\n",
    "r2 = r2_score(y_train, y_poly_pred)\n",
    "print(\"RMSE for Polynomial Regression: \", rmse)\n",
    "```\n",
    "\n",
    "Output:\n",
    "    - RMSE for Linear Regression:     6.772942\n",
    "    - RMSE for Polynomial Regression: 6.420495\n",
    "\n",
    "We can see that the RMSE of polynomial regression is slightly better than that of linear regression. Therefore, she will be the preferred model in this step.\n",
    "\n",
    "**6. Model tuning**\n",
    "Finding the best combination of hyperparameters of a model can be treated as a search problem. Such a search exercise is normally known as *model tuning* and is one of the most important steps in model development. Understands the search for the best model parameters when using techniques such as *grid search*. You create a grid with all possible combinations of hyperparameters and train the model using each of them. In addition to grid search, there are several other techniques for model tuning, including randomized search, Bayesian and hyperbrand optimization.\n",
    "\n",
    "Continuing our example, we have the polynomial as the best model: next, we will perform a grid search for the model, readjusting the polynomial regression with different degrees. We will compare the RMSE results for all models:\n",
    "\n",
    "```Python\n",
    "Deg = [1, 2, 3, 6, 10]\n",
    "results = []\n",
    "names = []\n",
    "for deg in Deg:\n",
    "    polynomial_features = PolynomialFeatures(degree=deg)\n",
    "    x_poly = polynomial_features.fit_transform(X_train)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit_transform(x_poly, y_train)\n",
    "    y_poly_pred = model.predict(x_poly)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_train, y_poly_pred))\n",
    "    r2 = r2_score(y_train, y_poly_pred)\n",
    "    results.append(rmse)\n",
    "    names.append(deg)\n",
    "\n",
    "plt.plot(names, results, 'o')\n",
    "plt.subtitle(\"Algorithm Comparison\")\n",
    "```\n",
    "\n",
    "The RMSE decreases as the degree increases, and the lowest RMSE is for the model with degree 10. However, models with degrees lower than 10 performed well, and the test set will be used to finalize the best model.\n",
    "\n",
    "**7. Finish the model**\n",
    "Here, we will perform the final steps for mold selection. First, we will run the predictions on the test dataset with the trained model. Then, we will try to understand the intuition of the model and save it for future use.\n",
    "\n",
    "**7.1. Performance with the test data set**\n",
    "The model selected during the training steps is evaluated again with the test set. This set allows us to compare different models in an unbiased way by basing our comparisons on data that was not used in any part of the training.\n",
    "\n",
    "```Python\n",
    "Deg = [1, 2, 3, 6, 8, 10]\n",
    "for deg in Deg:\n",
    "    polynomial_features = PolynomialFeatures(degree=deg)\n",
    "    x_poly = polynomial_features.fit_transform(X_train)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y_train)\n",
    "    x_poly_test = polynomial_features.fit_transform(X_test)\n",
    "    y_poly_pred_test = model.predict(x_poly_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_poly_pred_test))\n",
    "    r2 = r2_score(y_test, y_poly_pred_test)\n",
    "    results_test.append(rmse)\n",
    "    names_test.append(deg)\n",
    "    \n",
    "plt.plot(names_test, results_test, 'o')\n",
    "plt.subtitle(\"Algorithm Comparison\")\n",
    "```\n",
    "\n",
    "In the training set, we saw that the RMSE decreases with an increase in the degree of the polynomial model, and the polynomial of degree 10 had the lowest RMSE. However, as shown in the previous output for polynomial of degree 10, although the training set achieved the best results, the results on the test set are poor. For the polynomial of degree 8, the RMSE on the test set is relatively higher. The polynomial of degree 6 shows the best result in the test set (although the difference is small when compared with other polynomials of lower degrees in the test set), as well as good results in the training set. For these reasons, it is the preferred model.\n",
    "\n",
    "In addition to model performance, there are several other factors to consider when selecting a model, such as simplicity, interpretability and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de659b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
